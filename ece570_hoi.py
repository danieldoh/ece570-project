# -*- coding: utf-8 -*-
"""ECE57000 Project - Temporally Consistent Amodal Completion for Human-Object Interaction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IruT9jFnXXjArqyk8bD0oap4VMeoQ7zv

## Import
"""

!pip install --upgrade huggingface_hub -q
!pip install -qq -U diffusers==0.31.0 transformers ftfy gradio accelerate -q
!pip install ftfy regex tqdm -q
!pip install git+https://github.com/openai/CLIP.git -q

!pip install ipdb -q

import os
import cv2
import ipdb
import glob
import math
import clip
import random
import inspect
import numpy as np
import gradio as gr
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
from torch.utils.data import DataLoader, Dataset

from tqdm import tqdm
from PIL import Image
from typing import List, Optional, Union
from transformers import CLIPProcessor, CLIPModel

from diffusers import StableDiffusionInpaintPipeline

dataset_dir = "/content/drive/MyDrive/ECE570/project"

from google.colab import drive
drive.mount('/content/drive')

device = "cuda"

pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-inpainting",
    torch_dtype=torch.float16,
).to("cuda")

generator = torch.Generator(device=device).manual_seed(42)

"""## Dataset Preprocessing

### Crop
"""

class FrameDataset(Dataset):
    def __init__(self, frame_list, frame_dir, device):
        self.frame_list = frame_list
        self.frame_dir = frame_dir
        self.device = device

    def __len__(self):
        return len(self.frame_list)

    def __getitem__(self, idx):
        frame = self.frame_list[idx]
        frame_path = os.path.join(self.frame_dir, frame, f"{frame}.color_512.jpg")

        image = Image.open(frame_path).convert("RGB")

        pipe = StableDiffusionInpaintPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-inpainting",
            torch_dtype=torch.float16
        ).to(self.device)

        image_processed = pipe.image_processor.preprocess(
            image, height=512, width=512
        ).to(self.device, dtype=torch.float16)
        return image_processed, frame

def compute_bbox_for_frame(args):
    """
    For a single frame, load the person + obj masks,
    compute the bounding box (bmin, bmax), and return it.
    """
    frame_dir, f, threshold = args
    obj_path = os.path.join(frame_dir, f, f"{f}.obj_mask.png")
    person_path = os.path.join(frame_dir, f, f"{f}.person_mask.png")

    obj_mask = np.array(Image.open(obj_path))
    person_mask = np.array(Image.open(person_path))

    bmin, bmax = masks2bbox([person_mask, obj_mask], threshold=threshold)
    return bmin, bmax

def compute_global_bbox(frame_dir, frame_list, threshold=127, bbox_exp=1.0):
    """
    Parallel version that:
      1) Distributes frames to worker processes to compute per-frame bboxes
      2) Accumulates them into one global bmin, bmax
      3) Returns the final global_center, global_crop_size
    """
    global_bmin = np.array([np.inf, np.inf], dtype=float)
    global_bmax = np.array([-np.inf, -np.inf], dtype=float)

    num_workers = min(cpu_count(), 8)
    with Pool(num_workers) as p:
        results = p.map(
            compute_bbox_for_frame,
            [(frame_dir, f, threshold) for f in frame_list]
        )

    for bmin, bmax in results:
        bmin = bmin.astype(float)
        bmax = bmax.astype(float)
        global_bmin = np.minimum(global_bmin, bmin)
        global_bmax = np.maximum(global_bmax, bmax)

    global_center = (global_bmin + global_bmax) / 2.0
    width = global_bmax[0] - global_bmin[0]
    height = global_bmax[1] - global_bmin[1]
    crop_size = max(width, height) * bbox_exp

    crop_size = int(np.ceil(crop_size))
    if crop_size % 2 == 1:
        crop_size += 1
    global_center = global_center.astype(int)

    return global_center, crop_size

def masks2bbox(masks, threshold=127):
    """

    :param masks:
    :param threshold:
    :return: bounding box corner coordinate
    """
    mask_comb = np.zeros_like(masks[0], dtype=bool)
    for m in masks:
        mask_comb = mask_comb | (m > threshold)

    yid, xid = np.where(mask_comb)
    bmin = np.array([xid.min(), yid.min()])
    bmax = np.array([xid.max(), yid.max()])
    return bmin, bmax

def get_crop_params(mask_hum, mask_obj, bbox_exp=1.0):
    "compute bounding box based on masks"
    bmin, bmax = masks2bbox([mask_hum, mask_obj])
    crop_center = (bmin + bmax) // 2
    # crop_size = np.max(bmax - bmin)
    crop_size = int(np.max(bmax - bmin) * bbox_exp)
    if crop_size % 2 == 1:
        crop_size += 1  # make sure it is an even number
    return bmax, bmin, crop_center, crop_size

def crop(img, center, crop_size):
    """
    Crop a square region from 'img' of size 'crop_size' around 'center'.
    If the crop would extend outside the image, clamp and pad if needed.
    """
    assert isinstance(img, np.ndarray)
    h, w = img.shape[:2]

    # Ensure we don't request more than the image size
    crop_size = int(crop_size)
    crop_size = min(crop_size, w)  # don't exceed image width
    crop_size = min(crop_size, h)  # don't exceed image height

    half = crop_size // 2
    cx, cy = int(center[0]), int(center[1])

    x1 = cx - half
    y1 = cy - half
    x2 = x1 + crop_size
    y2 = y1 + crop_size

    # Clamp so the region stays within [0, w) and [0, h)
    if x1 < 0:
        x1 = 0
        x2 = crop_size
    elif x2 > w:
        x2 = w
        x1 = w - crop_size

    if y1 < 0:
        y1 = 0
        y2 = crop_size
    elif y2 > h:
        y2 = h
        y1 = h - crop_size

    # Now we have a square region [x1:x2, y1:y2] that fits in the image
    cropped = img[y1:y2, x1:x2]

    # If for some reason the subregion is smaller than crop_size (image < crop_size):
    # pad the difference
    pad_h = crop_size - (y2 - y1)
    pad_w = crop_size - (x2 - x1)

    if pad_h > 0 or pad_w > 0:
        if len(img.shape) == 3:
            cropped = np.pad(
                cropped,
                pad_width=((0,pad_h), (0,pad_w), (0,0)),
                mode='constant',
                constant_values=0
            )
        else:
            cropped = np.pad(
                cropped,
                pad_width=((0,pad_h), (0,pad_w)),
                mode='constant',
                constant_values=0
            )
    return cropped

def resize(img, img_size, mode=cv2.INTER_LINEAR):
    """
    resize image to the input
    :param img:
    :param img_size: (width, height) of the target image size
    :param mode:
    :return:
    """
    h, w = img.shape[:2]
    load_ratio = 1.0 * w / h
    netin_ratio = 1.0 * img_size[0] / img_size[1]
    assert load_ratio == netin_ratio, "image aspect ration not matching, given image: {}, net input: {}".format(
        img.shape, img_size)
    resized = cv2.resize(img, img_size, interpolation=mode)
    return resized


def extract_all_frames(video_path: str, output_folder: str) -> None:
    os.makedirs(output_folder, exist_ok=True)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise IOError(f"Cannot open video file {video_path}")

    frame_idx = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        filename = os.path.join(output_folder, f"frame_{frame_idx:05d}.jpg")
        cv2.imwrite(filename, frame)
        frame_idx += 1

    cap.release()
    print(f"Extracted {frame_idx} frames to '{output_folder}'.")



"""### Data Import"""

video_name = [
    "Date03_Sub03_tablesquare_lift",
    "Date03_Sub04_chairwood_sit",
    "Date03_Sub03_plasticcontainer"
]

video_prompt = [
    "square table",
    "wood chair",
    "plastic container"
]

def create_object_only_mask(video_dir):
    frame_dir = os.path.join(video_dir, "frame")
    object_mask_dir = os.path.join(video_dir, "mask")

    frame_list = sorted(os.listdir(frame_dir))
    object_mask_list = sorted(os.listdir(object_mask_dir))

    output_dir = os.path.join(video_dir, "object_only_mask")
    os.makedirs(output_dir, exist_ok=True)

    object_only_list = []
    for frame, mask in zip(frame_list, object_mask_list):
        img = Image.open(os.path.join(frame_dir, frame))
        obj_mask = Image.open(os.path.join(object_mask_dir, mask))

        img_np = np.array(img, dtype=np.float32)
        obj_mask_np = np.array(obj_mask, dtype=np.float32)

        binary_mask = (obj_mask_np > 0).astype(np.float32)
        binary_mask = binary_mask[:, :, None]

        img_np = img_np * binary_mask
        img_np = img_np.astype(np.uint8)
        img_np = Image.fromarray(img_np)

        frame_number = frame.split(".")[0]
        img_np.save(os.path.join(output_dir, f"{frame_number}.obj_mask.png"))

    return object_only_list

for video in tqdm(video_name, total=len(video_name), desc="Processing video"):
    create_object_only_mask(os.path.join(dataset_dir, video))

"""## Feature Extraction"""

def feature_extraction(pipe, generator, frame_dir, output_dir):

    def image_process(frame):
        processed = pipe.image_processor.preprocess(frame, height=512, width=512).to(
            device=device,
            dtype=torch.float16
        )

        return processed

    frame_list = sorted(os.listdir(frame_dir))

    # import ipdb; ipdb.set_trace()

    output_path = os.path.join(output_dir, "features.npy")

    frame_features = np.empty((len(frame_list), 4, 64, 64), dtype=np.float16)

    with torch.no_grad():
        for i, frame in tqdm(enumerate(frame_list), total=len(frame_list), desc="Extracting Features"):
            frame_path = os.path.join(frame_dir, frame)
            pil_frame = Image.open(frame_path)
            image_processed = image_process(pil_frame)
            features = pipe._encode_vae_image(image_processed, generator)
            frame_features[i] = features.cpu().numpy()

    print(frame_features.shape)
    print(frame_features[0].shape)

    np.save(output_path, frame_features, allow_pickle=False, fix_imports=False)

    print(f"Extracted features saved at {output_path}")

for video in video_name:
    frame_dir = os.path.join(dataset_dir, video, "frame")
    output_dir = os.path.join(dataset_dir, video)

    feature_extraction(pipe, generator, frame_dir, output_dir)

feature_path = os.path.join(dataset_dir, video_name[0], "features.npy")
features = np.load(feature_path, allow_pickle=False)

print("Loaded feature array shape:", features.shape)

frame_idx = 0
print("Frame 0 feature map shape:", features[frame_idx].shape)

feat = features[frame_idx]   # shape (4, 64, 64)

# 1) Plot each channel as its own figure
for ch in range(feat.shape[0]):
    plt.figure()
    plt.imshow(feat[ch])          # default colormap
    plt.title(f"Frame {frame_idx}, Channel {ch}")
    plt.colorbar()                # show scale
    plt.xlabel("Width")
    plt.ylabel("Height")
    plt.show()

"""## Occlusion Region Identification"""

occluded_mask_dict = {}

for video in video_name:
    occlusion_mask_dir = os.path.join(dataset_dir, video, "occlusion_mask")
    os.makedirs(occlusion_mask_dir, exist_ok=True)

    object_gt_mask_dir = os.path.join(dataset_dir, video, "gt_mask")
    human_mask_dir = os.path.join(dataset_dir, video, "human")

    object_gt_mask_list = sorted(os.listdir(object_gt_mask_dir))
    human_mask_list = sorted(os.listdir(human_mask_dir))

    occluded_mask_list = []
    for obj, human in tqdm(zip(object_gt_mask_list, human_mask_list), total=len(object_gt_mask_list), desc="Finding occlusion"):
      obj_image = Image.open(os.path.join(object_gt_mask_dir, obj))
      human_image = Image.open(os.path.join(human_mask_dir, human))

      obj_np = np.array(obj_image)
      human_np = np.array(human_image)

      obj_np = obj_np.astype(np.uint8)
      human_np = human_np.astype(np.uint8)

      intersection = np.bitwise_and(obj_np, human_np)

      intersection = Image.fromarray(intersection)
      intersection = intersection.convert("L")

      frame_number = obj.split(".")[0]
      intersection.save(os.path.join(occlusion_mask_dir, f"{frame_number}.occlu_mask.png"))

      occluded_mask_list.append(intersection)

    occluded_mask_dict[video] = occluded_mask_list

for video in video_name:
  frame_dir          = os.path.join(dataset_dir, video, "frame")
  object_gt_mask_dir = os.path.join(dataset_dir, video, "gt_mask")
  human_mask_dir     = os.path.join(dataset_dir, video, "human")
  occlu_mask_dir     = os.path.join(dataset_dir, video, "occlusion_mask")

  frame_files = sorted(os.listdir(frame_dir))
  gt_mask_files = sorted(os.listdir(object_gt_mask_dir))
  human_mask_files = sorted(os.listdir(human_mask_dir))
  occlu_mask_files = sorted(os.listdir(occlu_mask_dir))

  n_frames    = len(frame_files)

  step           = 10
  selected_idxs  = list(range(0, n_frames, step))
  n_rows         = len(selected_idxs)

  fig, axes = plt.subplots(n_rows, 4, figsize=(12, 3 * n_rows))

  fig.suptitle(f"Video: {video}", fontsize=18)


  for row, idx in enumerate(selected_idxs):
      frame_path      = os.path.join(frame_dir, frame_files[idx])
      obj_mask_path   = os.path.join(object_gt_mask_dir, gt_mask_files[idx])
      human_mask_path = os.path.join(human_mask_dir, human_mask_files[idx])
      occlu_path      = os.path.join(occlu_mask_dir, occlu_mask_files[idx])

      frame_img      = Image.open(frame_path)
      obj_mask_img   = Image.open(obj_mask_path)
      human_mask_img = Image.open(human_mask_path)
      occlu_img      = Image.open(occlu_path)

      axes[row, 0].imshow(frame_img)
      axes[row, 0].set_title(f"Frame {idx}")
      axes[row, 1].imshow(obj_mask_img, cmap="gray")
      axes[row, 1].set_title("Object GT Mask")
      axes[row, 2].imshow(human_mask_img, cmap="gray")
      axes[row, 2].set_title("Human Mask")
      axes[row, 3].imshow(occlu_img, cmap="gray")
      axes[row, 3].set_title("Occlusion Mask")

      for col in range(4):
          axes[row, col].axis("off")

  plt.tight_layout(rect=[0, 0, 1, 0.96])
  plt.show()

"""## Cross-Attention"""

class CrossAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

    def forward(self, xQ, xKV_list, mask=None):

        B, Nq, C = xQ.shape
        Nk = Nq

        Q = xQ.view(B, Nq, self.num_heads, self.head_dim).transpose(1, 2)

        scores_list = []
        for K in xKV_list:

            K = K.view(B, Nk, self.num_heads, self.head_dim).transpose(1, 2)
            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
            scores_list.append(scores)

        scores = torch.cat(scores_list, dim=-1)

        attn_weights = torch.softmax(scores, dim=-1)

        attn_weights_chunks = torch.chunk(attn_weights, len(xKV_list), dim=-1)

        context_list = []
        for attn_w_chunk, V in zip(attn_weights_chunks, xKV_list):
            V = V.view(B, Nk, self.num_heads, self.head_dim).transpose(1, 2)
            context_i = torch.matmul(attn_w_chunk, V)
            context_list.append(context_i)

        context = sum(context_list)

        context = context.transpose(1, 2).contiguous().view(B, Nq, C)

        return context, attn_weights

def feature_fusion(dataset_dir,
                    features_path,
                    output_path,
                    start_idx,
                    end_idx,
                    device):

    feat_np = np.load(features_path)
    feat = torch.from_numpy(feat_np).float().to(device)
    N, C, H, W = feat.shape

    gt_mask_dir    = os.path.join(dataset_dir, "gt_mask")
    all_masks      = sorted(os.listdir(gt_mask_dir))
    selected_masks = all_masks[start_idx:end_idx]

    mask_list = []
    for mask_fname in selected_masks:
        m = Image.open(os.path.join(gt_mask_dir, mask_fname)).convert("L")
        m = m.resize((W, H), Image.Resampling.LANCZOS)
        m = np.array(m, dtype=np.float32) / 255.0
        m = torch.from_numpy(m).unsqueeze(0).unsqueeze(0)
        mask_list.append(m.to(device))

    cross_attn = CrossAttention(embed_dim=C, num_heads=1).to(device)

    fused_list = []

    for i, m in enumerate(tqdm(mask_list, desc="Fusing features")):
        t = start_idx + i
        xQ = feat[t].unsqueeze(0)

        kv_idxs = [min(max(t + dt, 0), N - 1) for dt in (-3, -2, -1, +1, +2, +3)]
        xKV_list = [feat[idx].unsqueeze(0) for idx in kv_idxs]

        B, _, _, _ = xQ.shape
        Nq = H * W
        xQ_2D      = xQ.view(B, C, Nq).permute(0, 2, 1)
        xKV_2D_lst = [xKV.view(B, C, Nq).permute(0, 2, 1)
                       for xKV in xKV_list]

        out_2D, _ = cross_attn(xQ_2D, xKV_2D_lst, mask=None)
        out       = out_2D.permute(0, 2, 1).view(B, C, H, W)

        m_bin       = (m >= 0.5).float()
        out_masked  = out * m_bin

        fused_list.append(out_masked)

    fused_tensor = torch.cat(fused_list, dim=0)
    torch.save({
        'feature_fused_masked': fused_tensor.cpu(),
        'feature_fused_masked_shape': fused_tensor.shape
    }, output_path)

    print(f"Saved fused+masked features at {output_path}, shape {fused_tensor.shape}")

for video in tqdm(video_name, total=len(video_name), desc="Processing attention"):

  feature_fusion(
      dataset_dir=os.path.join(dataset_dir, video),
      features_path=os.path.join(dataset_dir, video, "features.npy"),
      output_path=os.path.join(dataset_dir, video, "features_fused_masked.pt"),
      start_idx=0,
      end_idx=60,
      device=device
  )

"""## Inpainting - Ours"""

def get_attn_feature(path):
    return torch.load(path)['feature_fused_masked']

def get_occluded_mask(video_dir):
    occluded_mask_dir = os.path.join(video_dir, "occlusion_mask")
    occluded_mask_list = []
    mask_list = sorted(os.listdir(occluded_mask_dir))

    for frame in mask_list:
        occluded_mask_path = os.path.join(occluded_mask_dir, frame)
        occluded_mask = Image.open(occluded_mask_path)
        occluded_mask_list.append(occluded_mask)

    return occluded_mask_list

def get_object_only_image(video_dir):
    object_only_dir = os.path.join(video_dir, "object_only_mask")
    object_only_list = []
    object_only_images = sorted(os.listdir(object_only_dir))

    for frame in object_only_images:
        if frame == "gt_mask":
          continue
        object_only_path = os.path.join(object_only_dir, frame)
        object_only = Image.open(object_only_path)
        object_only_list.append(object_only)

    return object_only_list

def get_frames_list(video_dir):
    frame_dir = os.path.join(video_dir, "frame")
    frame_list = sorted(os.listdir(frame_dir))

    return frame_list

def inpaint(pipe,
            video_dir,
            feat_fused_path,
            output_dir,
            prompt):

    pipe.safety_checker = None

    frame_list = get_frames_list(video_dir)
    feature_masked = get_attn_feature(feat_fused_path)
    occluded_mask_list = get_occluded_mask(video_dir)
    segmented_image = get_object_only_image(video_dir)
    # import ipdb; ipdb.set_trace()

    inpainted_result = []
    print(f" Output will be saved at: {output_dir}")
    print(f" Text Prompt: {prompt}")

    for t in tqdm(range(len(occluded_mask_list)), desc="Inpainting Progress", dynamic_ncols=True):
        prompt = prompt
        latent_feat = feature_masked[t].unsqueeze(0).half()
        input_img = segmented_image[t]
        mask_img = occluded_mask_list[t]
        strength = 0.9
        num_infer_steps = 50
        guid_scale = 7.5

        # print("latent feature shape: ", latent_feat.shape)
        # print("latent feature type: ", type(latent_feat))
        # print("latent feature data type: ", latent_feat.dtype)
        # print("input image type: ", type(input_img))
        # print("mask image type: ", type(mask_img))

        inpainted_image = pipe(prompt=prompt,
                               image=input_img,
                               mask_image=mask_img,
                               strength=strength,
                               latents=latent_feat,
                               num_inference_steps=num_infer_steps,
                               guidance_scale=guid_scale
                               ).images[0]

        frame_number = frame_list[t].split(".")[0]
        output_path = os.path.join(output_dir, f"{frame_number}.inpainted.jpg")
        inpainted_image.save(output_path)

        # print(f"Inpainted results are saved at {output_path}")

for i, video in tqdm(enumerate(video_name[2:]), total=len(video_name), desc="Processing inpainting"):
    video_dir = os.path.join(dataset_dir, video)
    feat_fused_path = os.path.join(dataset_dir, video, "features_fused_masked.pt")
    output_dir = os.path.join(dataset_dir, video, "inpainted")
    prompt = video_prompt[i]
    os.makedirs(output_dir, exist_ok=True)
    inpaint(pipe, video_dir, feat_fused_path, output_dir, prompt)

"""## Inpainting - Stable Diffusion with Random noise"""

def inpaint(pipe,
            video_dirh,
            generator,
            output_dir,
            prompt):

    pipe.safety_checker = None

    frame_list = get_frames_list(video_dir)
    occluded_mask_list = get_occluded_mask(video_dir)
    segmented_image = get_object_only_image(video_dir)

    inpainted_result = []
    print(f" Output will be saved at: {output_dir}")
    print(f" Text Prompt: {prompt}")

    for t in tqdm(range(len(occluded_mask_list)), desc="Inpainting Progress", dynamic_ncols=True):
        prompt = prompt
        input_img = segmented_image[t]
        mask_img = occluded_mask_list[t]
        strength = 0.9
        num_infer_steps = 50
        guid_scale = 7.5

        # print("latent feature shape: ", latent_feat.shape)
        # print("latent feature type: ", type(latent_feat))
        # print("latent feature data type: ", latent_feat.dtype)
        # print("input image type: ", type(input_img))
        # print("mask image type: ", type(mask_img))

        inpainted_image = pipe(prompt=prompt,
                               image=input_img,
                               mask_image=mask_img,
                               strength=strength,
                               generator=generator,
                               num_inference_steps=num_infer_steps,
                               guidance_scale=guid_scale
                               ).images[0]

        frame_number = frame_list[t].split(".")[0]
        output_path = os.path.join(output_dir, f"{frame_number}.inpainted.jpg")
        inpainted_image.save(output_path)

        # print(f"Inpainted results are saved at {output_path}")

for i, video in tqdm(enumerate(video_name), total=len(video_name), desc="Processing inpainting"):
    video_dir = os.path.join(dataset_dir, video)
    generator = torch.Generator(device=device).manual_seed(42)
    output_dir = os.path.join(dataset_dir, video, "inpainted_random")
    prompt = video_prompt[i]
    os.makedirs(output_dir, exist_ok=True)
    inpaint(pipe, video_dir, generator, output_dir, prompt)

"""## Visualization"""

for video in video_name:
  frame_dir          = os.path.join(dataset_dir, video, "frame")
  occlu_mask_dir     = os.path.join(dataset_dir, video, "occlusion_mask")
  inpainted_dir     = os.path.join(dataset_dir, video, "inpainted")

  frame_files = sorted(os.listdir(frame_dir))
  occlu_mask_files = sorted(os.listdir(occlu_mask_dir))
  inpainted_files = sorted(os.listdir(inpainted_dir))

  n_frames    = len(frame_files)

  step           = 10
  selected_idxs  = list(range(0, n_frames, step))
  n_rows         = len(selected_idxs)

  fig, axes = plt.subplots(n_rows, 3, figsize=(12, 3 * n_rows))

  fig.suptitle(f"Video: {video}", fontsize=18)


  for row, idx in enumerate(selected_idxs):
      frame_path      = os.path.join(frame_dir, frame_files[idx])
      occlu_path      = os.path.join(occlu_mask_dir, occlu_mask_files[idx])
      inpainted_path      = os.path.join(inpainted_dir, inpainted_files[idx])

      frame_img      = Image.open(frame_path)
      occlu_img      = Image.open(occlu_path)
      inpainted      = Image.open(inpainted_path)

      axes[row, 0].imshow(frame_img)
      axes[row, 0].set_title(f"Frame {idx}")
      axes[row, 1].imshow(occlu_img, cmap="gray")
      axes[row, 1].set_title("Occlusion Mask")
      axes[row, 2].imshow(inpainted)
      axes[row, 2].set_title("Inpainted")

      for col in range(3):
          axes[row, col].axis("off")

  plt.tight_layout(rect=[0, 0, 1, 0.96])
  plt.show()

for video in video_name:
  frame_dir          = os.path.join(dataset_dir, video, "frame")
  occlu_mask_dir     = os.path.join(dataset_dir, video, "occlusion_mask")
  inpainted_dir     = os.path.join(dataset_dir, video, "inpainted_random")

  frame_files = sorted(os.listdir(frame_dir))
  occlu_mask_files = sorted(os.listdir(occlu_mask_dir))
  inpainted_files = sorted(os.listdir(inpainted_dir))

  n_frames    = len(frame_files)

  step           = 10
  selected_idxs  = list(range(0, n_frames, step))
  n_rows         = len(selected_idxs)

  fig, axes = plt.subplots(n_rows, 3, figsize=(12, 3 * n_rows))

  fig.suptitle(f"Video: {video}", fontsize=18)


  for row, idx in enumerate(selected_idxs):
      frame_path      = os.path.join(frame_dir, frame_files[idx])
      occlu_path      = os.path.join(occlu_mask_dir, occlu_mask_files[idx])
      inpainted_path      = os.path.join(inpainted_dir, inpainted_files[idx])

      frame_img      = Image.open(frame_path)
      occlu_img      = Image.open(occlu_path)
      inpainted_random = Image.open(inpainted_path)

      axes[row, 0].imshow(frame_img)
      axes[row, 0].set_title(f"Frame {idx}")
      axes[row, 1].imshow(occlu_img, cmap="gray")
      axes[row, 1].set_title("Occlusion Mask")
      axes[row, 2].imshow(inpainted_random)
      axes[row, 2].set_title("Inpainted_random")

      for col in range(3):
          axes[row, col].axis("off")

  plt.tight_layout(rect=[0, 0, 1, 0.96])
  plt.show()

"""## Evaluation

### Generating Segmented Mask
"""

for video in video_name:
    video_dir = os.path.join(dataset_dir, video)
    inpainted = sorted(os.listdir(os.path.join(video_dir, "inpainted")))

    output_dir = os.path.join(video_dir, "inpainted_mask_final")
    os.makedirs(output_dir, exist_ok=True)

    for frame in inpainted:
        inpainted_path = os.path.join(video_dir, "inpainted", frame)
        image = Image.open(inpainted_path)
        image_gray = image.convert('L')

        threshold = 20
        image_binary = image_gray.point(lambda p: 255 if p > threshold else 0, '1')

        frame_number = frame.split(".")[0]
        output_path = os.path.join(output_dir, f"{frame_number}.inpainted_mask.png")
        image_binary.save(output_path)
        img_np = np.array(image_binary)
        # import ipdb; ipdb.set_trace()

for video in video_name:
    video_dir = os.path.join(dataset_dir, video)
    inpainted = sorted(os.listdir(os.path.join(video_dir, "inpainted_random")))

    output_dir = os.path.join(video_dir, "inpainted_mask_final")
    os.makedirs(output_dir, exist_ok=True)

    for frame in inpainted:
        inpainted_path = os.path.join(video_dir, "inpainted_random", frame)
        image = Image.open(inpainted_path)
        image_gray = image.convert('L')

        threshold = 20
        image_binary = image_gray.point(lambda p: 255 if p > threshold else 0, '1')

        frame_number = frame.split(".")[0]
        output_path = os.path.join(output_dir, f"{frame_number}.inpainted_mask.png")
        image_binary.save(output_path)
        img_np = np.array(image_binary)
        # import ipdb; ipdb.set_trace()

def compute_clip_score(model, preprocess, video_dir, text_description, inpainted_dir):
    clip_scores = []

    inpainted_dir = "inpainted_random"
    inpainted_video_dir = os.path.join(video_dir, inpainted_dir)
    inpainted_frames = sorted(os.listdir(inpainted_video_dir))
    # import ipdb; ipdb.set_trace()
    for frame in inpainted_frames:
        image_path = os.path.join(inpainted_video_dir, frame)

        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
        text = clip.tokenize([text_description]).to(device)

        with torch.no_grad():
            image_features = model.encode_image(image)
            text_features = model.encode_text(text)

            image_features /= image_features.norm(dim=-1, keepdim=True)
            text_features /= text_features.norm(dim=-1, keepdim=True)

            clip_scores.append((image_features @ text_features.T).item())

    average_clip_score = np.sum(clip_scores) / len(clip_scores)

    return average_clip_score

def compute_iou(video_dir, inpainted_dir):
    iou_scores = []

    inpainted_dir = "inpainted_random"
    inpainted_video_dir = os.path.join(video_dir, inpainted_dir)
    inpainted_frames = sorted(os.listdir(inpainted_video_dir))

    for frame in inpainted_frames:
        frame_number = frame.split(".")[0]
        gt_path = os.path.join(video_dir, "gt_mask", f"{frame_number}.obj_rend_full_512.png")
        inpainted_path = os.path.join(video_dir, "inpainted_mask_final", f"{frame_number}.inpainted_mask.png")

        gt_img = Image.open(gt_path)
        inpainted_img = Image.open(inpainted_path)

        gt = np.array(gt_img)
        inpainted = np.array(inpainted_img)

        gt_mask_bool = gt.astype(bool)
        pred_mask_bool = inpainted.astype(bool)

        intersection = np.logical_and(pred_mask_bool, gt_mask_bool).sum()
        union = np.logical_or(pred_mask_bool, gt_mask_bool).sum()

        if union == 0:
            iou_scores.append(1.0 if intersection == 0 else 0.0)
            continue

        iou = intersection / union
        iou_scores.append(iou)

    average_iou_score = np.sum(iou_scores) / len(iou_scores)

    return average_iou_score

def clip_embedding(model, preprocess, video_dir, inpainted_dir):

    all_embeddings = []

    inpainted_dir = "inpainted_random"
    inpainted_video_dir = os.path.join(video_dir, inpainted_dir)
    inpainted_frames = sorted(os.listdir(inpainted_video_dir))
    for frame in inpainted_frames:

        image_path = os.path.join(inpainted_video_dir, frame)
        pil_image = Image.open(image_path)
        image_input = preprocess(pil_image).unsqueeze(0).to(device)

        with torch.no_grad():
            embedding = model.encode_image(image_input)
            embedding = embedding / embedding.norm(dim=-1, keepdim=True)

        all_embeddings.append(embedding.squeeze(0))

    return all_embeddings

def cosine_sim(all_embeddings):
    cos_sim = torch.nn.CosineSimilarity(dim=-1)
    similarities = []

    for i in range(len(all_embeddings) - 1):
        sim = cos_sim(all_embeddings[i], all_embeddings[i+1])
        similarities.append(sim.item())

    similarities = np.array(similarities)

    avg_similarity = np.sum(similarities) / len(similarities)

    return float(avg_similarity)


def compute_tc_score(model, preprocess, video_dir, inpainted_dir):
    all_embeddings = clip_embedding(model, preprocess, video_dir, inpainted_dir)
    avg_cosine_sim = cosine_sim(all_embeddings)

    return avg_cosine_sim

avg_clip = []
avg_iou = []
avg_tc = []
inpainted_dir = "inpainted"

for video in video_name:
    video_dir = os.path.join(dataset_dir, video)
    text_description = video_prompt[i]
    model, preprocess = clip.load("ViT-B/32", device=device)

    clip_score = compute_clip_score(model, preprocess, video_dir, text_description, inpainted_dir)
    iou_score = compute_iou(video_dir, inpainted_dir)
    tc_score = compute_tc_score(model, preprocess, video_dir, inpainted_dir)

    print(f"Video: {video}")
    print(f"CLIP Score: {clip_score}")
    print(f"IOU Score: {iou_score}")
    print(f"TC Score: {tc_score}")

    with open("results.txt", "a") as f:
        f.write(f"Video: {video}\n")
        f.write(f"CLIP Score: {clip_score}\n")
        f.write(f"IOU Score: {iou_score}\n")
        f.write(f"TC Score: {tc_score}\n")
        f.write("\n")

    avg_clip.append(clip_score)
    avg_iou.append(iou_score)
    avg_tc.append(tc_score)

print(f"Ours Average CLIP Score: {np.mean(avg_clip)}")
print(f"Ours Average IOU Score: {np.mean(avg_iou)}")
print(f"Ours Average TC Score: {np.mean(avg_tc)}")

random_avg_clip = []
random_avg_iou = []
random_avg_tc = []
inpainte_dir = "inpainted_random"

for video in video_name:
    video_dir = os.path.join(dataset_dir, video)
    # import ipdb; ipdb.set_trace()
    text_description = video_prompt[i]
    model, preprocess = clip.load("ViT-B/32", device=device)

    random_clip_score = compute_clip_score(model, preprocess, video_dir, text_description, inpainted_dir)
    random_iou_score = compute_iou(video_dir, inpainted_dir)
    random_tc_score = compute_tc_score(model, preprocess, video_dir, inpainted_dir)

    print(f"Video: {video}")
    print(f"CLIP Score: {random_clip_score}")
    print(f"IOU Score: {random_iou_score}")
    print(f"TC Score: {random_tc_score}")

    with open("results.txt", "a") as f:
        f.write(f"Video: {video}\n")
        f.write(f"CLIP Score: {random_clip_score}\n")
        f.write(f"IOU Score: {random_iou_score}\n")
        f.write(f"TC Score: {random_tc_score}\n")
        f.write("\n")

    random_avg_clip.append(random_clip_score)
    random_avg_iou.append(random_iou_score)
    random_avg_tc.append(random_tc_score)

print(f"SD Inpainting Average CLIP Score: {np.mean(random_avg_clip)}")
print(f"SD Inpainting Average IOU Score: {np.mean(random_avg_iou)}")
print(f"SD Inpainting Average TC Score: {np.mean(random_avg_tc)}")

